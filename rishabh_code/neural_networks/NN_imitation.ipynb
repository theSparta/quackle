{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Input, Lambda, Reshape\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Nadam\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend.tensorflow_backend import set_session\n",
    "from utils import limited_gpu_memory_session\n",
    "set_session(limited_gpu_memory_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath('./')\n",
    "CHECKPOINTED_WEIGHTS = os.path.join(DATA_DIR, 'checkpointed_weights.hdf5')\n",
    "INIT_WEIGHTS = os.path.join(DATA_DIR, 'init_weights_base.hdf5')\n",
    "EXPERIENCE_BUFFER_FILE = os.path.join(DATA_DIR, 'experience_buffer.p')\n",
    "MODEL_IMAGE = os.path.join(DATA_DIR, 'siamese_vgg16.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "def dense_relu_bn_dropout(x, size, dropout, alpha = 0.1, reg = 0):\n",
    "    x = Dense(size, kernel_regularizer = l2(reg))(x)\n",
    "    x = Activation('tanh')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def create_network(reg, dropout, alpha = 0.1):\n",
    "    inputs = Input(shape=(INPUT_SHAPE,))\n",
    "    x = dense_relu_bn_dropout(inputs, 8 , dropout, reg)\n",
    "    x = dense_relu_bn_dropout(x, 4, dropout, reg)\n",
    "    x = Dense(1)(x)\n",
    "    base_network = Model(inputs=inputs, outputs = x)\n",
    "    print(base_network.summary())\n",
    "    return base_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 161\n",
      "Trainable params: 137\n",
      "Non-trainable params: 24\n",
      "_________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 1)             161         input_2[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)            (None, 1)             0           model_1[1][0]                    \n",
      "                                                                   model_1[2][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 1)             0           subtract_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 161\n",
      "Trainable params: 137\n",
      "Non-trainable params: 24\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, merge, Input, Lambda, Reshape\n",
    "\n",
    "INPUT_SHAPE = 8\n",
    "\n",
    "base_network = create_network(reg = 0.5, dropout = 0.5)\n",
    "input_a = Input(shape=(INPUT_SHAPE,))\n",
    "processed_a = base_network(input_a)\n",
    "input_b = Input(shape=(INPUT_SHAPE,))\n",
    "processed_b = base_network(input_b)\n",
    "distance = layers.Subtract()([processed_a, processed_b])\n",
    "out = Activation('sigmoid')(distance)\n",
    "siamese_net = Model([input_a, input_b], out)\n",
    "    \n",
    "siamese_net.save_weights(INIT_WEIGHTS)\n",
    "print(siamese_net.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "MOVES = pickle.load(open(\"../moves_dict.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, item in MOVES.iteritems():\n",
    "    MOVES[key] = np.array(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in MOVES:\n",
    "    for x in MOVES[key]:\n",
    "        if x.shape[0] != 8:\n",
    "            print(x)\n",
    "        if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5,\n",
    "              patience=5, verbose = 1, min_lr=1e-8)\n",
    "early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                              min_delta=1e-4,\n",
    "                              patience=25,\n",
    "                              verbose=0, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=CHECKPOINTED_WEIGHTS, verbose=1, save_best_only=True, monitor='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nadam = Nadam(lr=1e-3)\n",
    "siamese_net.compile(optimizer=nadam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "siamese_net.load_weights(INIT_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience buffer loaded from /home/ubuntu/quackle/rishabh_code/neural_networks/experience_buffer.p\n",
      "Train: 2272842 Val: 94702\n"
     ]
    }
   ],
   "source": [
    "from utils import DataGenerator\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "load_from_file = os.path.exists(EXPERIENCE_BUFFER_FILE)\n",
    "save_to_file = not load_from_file\n",
    "datagen = DataGenerator(MOVES, batch_sz = BATCH_SIZE, load_from_file = load_from_file, \n",
    "                 save_to_file = save_to_file, file = EXPERIENCE_BUFFER_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4956Epoch 00000: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4956 - val_loss: 0.6932 - val_acc: 0.2120\n",
      "Epoch 2/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4965Epoch 00001: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4965 - val_loss: 0.6931 - val_acc: 0.2362\n",
      "Epoch 3/500\n",
      "4436/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4941Epoch 00002: val_acc did not improve\n",
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4941 - val_loss: 0.6932 - val_acc: 0.1735\n",
      "Epoch 4/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4933Epoch 00003: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4934 - val_loss: 0.6931 - val_acc: 0.2739\n",
      "Epoch 5/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4982Epoch 00004: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.2719\n",
      "Epoch 6/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4974Epoch 00005: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.0734\n",
      "Epoch 7/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4946Epoch 00006: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4946 - val_loss: 0.6930 - val_acc: 0.2924\n",
      "Epoch 8/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4979Epoch 00007: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.1543\n",
      "Epoch 9/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4953Epoch 00008: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4953 - val_loss: 0.6931 - val_acc: 0.1184\n",
      "Epoch 10/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4958Epoch 00009: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.1510\n",
      "Epoch 11/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4983Epoch 00010: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.2469\n",
      "Epoch 12/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4978Epoch 00011: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.2517\n",
      "Epoch 13/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4963\n",
      "Epoch 00012: reducing learning rate to 0.000500000023749.\n",
      "Epoch 00012: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4963 - val_loss: 0.6931 - val_acc: 0.2161\n",
      "Epoch 14/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4976Epoch 00013: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.1930\n",
      "Epoch 15/500\n",
      "4436/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4999Epoch 00014: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.2113\n",
      "Epoch 16/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4983Epoch 00015: val_acc did not improve\n",
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4983 - val_loss: 0.6931 - val_acc: 0.2079\n",
      "Epoch 17/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4971Epoch 00016: val_acc did not improve\n",
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.1880\n",
      "Epoch 18/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4968\n",
      "Epoch 00017: reducing learning rate to 0.000250000011874.\n",
      "Epoch 00017: val_acc did not improve\n",
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4969 - val_loss: 0.6932 - val_acc: 0.2048\n",
      "Epoch 19/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4982Epoch 00018: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.2036\n",
      "Epoch 20/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4981Epoch 00019: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.1669\n",
      "Epoch 21/500\n",
      "4434/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4973Epoch 00020: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.1906\n",
      "Epoch 22/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4971Epoch 00021: val_acc did not improve\n",
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4971 - val_loss: 0.6931 - val_acc: 0.2157\n",
      "Epoch 23/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4979Epoch 00022: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.3480\n",
      "Epoch 24/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4982Epoch 00023: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.2449\n",
      "Epoch 25/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4981Epoch 00024: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4981 - val_loss: 0.6931 - val_acc: 0.1940\n",
      "Epoch 26/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4950Epoch 00025: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.2779\n",
      "Epoch 27/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4948Epoch 00026: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.2691\n",
      "Epoch 28/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4981Epoch 00027: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.2429\n",
      "Epoch 29/500\n",
      "4436/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4976\n",
      "Epoch 00028: reducing learning rate to 0.000125000005937.\n",
      "Epoch 00028: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4976 - val_loss: 0.6931 - val_acc: 0.2005\n",
      "Epoch 30/500\n",
      "4433/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4968Epoch 00029: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4968 - val_loss: 0.6931 - val_acc: 0.2228\n",
      "Epoch 31/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4978Epoch 00030: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.2223\n",
      "Epoch 32/500\n",
      "4434/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4979Epoch 00031: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.2180\n",
      "Epoch 33/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4973Epoch 00032: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4439/4439 [==============================] - 31s - loss: 0.6931 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.2592\n",
      "Epoch 34/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4967\n",
      "Epoch 00033: reducing learning rate to 6.25000029686e-05.\n",
      "Epoch 00033: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.2609\n",
      "Epoch 35/500\n",
      "4434/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4961Epoch 00034: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.2700\n",
      "Epoch 36/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4966Epoch 00035: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.2693\n",
      "Epoch 37/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4961Epoch 00036: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4961 - val_loss: 0.6931 - val_acc: 0.2325\n",
      "Epoch 38/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4973Epoch 00037: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.2685\n",
      "Epoch 39/500\n",
      "4437/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4966\n",
      "Epoch 00038: reducing learning rate to 3.12500014843e-05.\n",
      "Epoch 00038: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4966 - val_loss: 0.6931 - val_acc: 0.2543\n",
      "Epoch 40/500\n",
      "4432/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4949Epoch 00039: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4949 - val_loss: 0.6931 - val_acc: 0.2482\n",
      "Epoch 41/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4948Epoch 00040: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.2174\n",
      "Epoch 42/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4950Epoch 00041: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4950 - val_loss: 0.6931 - val_acc: 0.2528\n",
      "Epoch 43/500\n",
      "4431/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4948Epoch 00042: val_acc did not improve\n",
      "4439/4439 [==============================] - 30s - loss: 0.6931 - acc: 0.4948 - val_loss: 0.6931 - val_acc: 0.2335\n",
      "Epoch 44/500\n",
      "4438/4439 [============================>.] - ETA: 0s - loss: 0.6931 - acc: 0.4956\n",
      "Epoch 00043: reducing learning rate to 1.56250007421e-05.\n",
      "Epoch 00043: val_acc did not improve\n",
      "4439/4439 [==============================] - 29s - loss: 0.6931 - acc: 0.4956 - val_loss: 0.6931 - val_acc: 0.2310\n",
      "Epoch 45/500\n",
      " 135/4439 [..............................] - ETA: 28s - loss: 0.6931 - acc: 0.4976"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_PAIRS, NUM_VAL_PAIRS = datagen.get_num_pairs()\n",
    "STEPS_PER_EPOCH = NUM_TRAIN_PAIRS//BATCH_SIZE\n",
    "VALIDATION_STEPS = NUM_VAL_PAIRS//BATCH_SIZE\n",
    "history = siamese_net.fit_generator(\n",
    "        datagen.next_train(),\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        epochs=500,\n",
    "        validation_data=datagen.next_val(),\n",
    "        validation_steps=VALIDATION_STEPS,\n",
    "        callbacks = [reduce_lr, checkpointer, early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
